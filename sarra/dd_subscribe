#!/usr/bin/python3
#
# This file is part of sarracenia.
# The sarracenia suite is Free and is proudly provided by the Government of Canada
# Copyright (C) Her Majesty The Queen in Right of Canada, Environment Canada, 2008-2015
#
# Questions or bugs report: dps-client@ec.gc.ca
# sarracenia repository: git://git.code.sf.net/p/metpx/git
# Documentation: http://metpx.sourceforge.net/#SarraDocumentation
#
# dd_subscribe    : python3 program allowing users to download product from dd.weather.gc.ca
#                   as soon as they are made available (through amqp notifications)
#
#
# Code contributed by:
#  Michel Grenier - Shared Services Canada
#  Jun Hu         - Shared Services Canada
#  Last Changed   : Sep 22 10:41:32 EDT 2015
#  Last Revision  : Sep 22 10:41:32 EDT 2015
#
########################################################################
#  This program is free software; you can redistribute it and/or modify
#  it under the terms of the GNU General Public License as published by
#  the Free Software Foundation; either version 2 of the License, or
#  (at your option) any later version.
#
#  This program is distributed in the hope that it will be useful, 
#  but WITHOUT ANY WARRANTY; without even the implied warranty of 
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the 
#  GNU General Public License for more details.
#
#  You should have received a copy of the GNU General Public License
#  along with this program; if not, write to the Free Software
#  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307  USA
#
#

import urllib, logging, logging.handlers, os, random, re, signal, string, sys, time, getopt
import amqplib.client_0_8 as amqp
import stat, os.path
import calendar,socket,urllib.parse
from hashlib import md5

#============================================================
# usage example
#
# dd_subscribe configfile.conf
#
#============================================================

try   : import amqplib.client_0_8 as amqp
except: pass

try   : import pika
except: pass

import logging, sys, time


# ==========
# message to mimic amqplib from pika
# ==========

class MY_Message:

   def __init__(self,logger):
       self.logger        = logger
       self.delivery_info = {}
       self.properties    = {}

   def pika_to_amqplib(self, method_frame, properties, body ):
       try :
               self.body  = body

               self.delivery_info['exchange']         = method_frame.exchange
               self.delivery_info['routing_key']      = method_frame.routing_key
               self.delivery_tag                      = method_frame.delivery_tag

               self.properties['application_headers'] = properties.headers
       except:
               (stype, value, tb) = sys.exc_info()
               self.logger.error("pika_to_amqp_publish Type: %s, Value: %s" % (stype, value))
               self.logger.error("in pika to amqplib %s %s" %(vars(method_frame),vars(properties)))


# ==========
# HostConnect
# ==========

class HostConnect:

   def __init__(self, logger = None):

       self.asleep     = False
       self.loop       = True

       self.connection = None
       self.channel    = None
       self.ssl        = True

       self.logger     = logger

       self.protocol   = 'amqp'
       self.host       = 'localhost'
       self.port       = None
       self.user       = 'guest'
       self.passwd     = 'guest'

       self.rebuilds   = []
       self.toclose    = []

       self.sleeping   = None

       self.pika_available    = 'pika'    in sys.modules
       self.amqplib_available = 'amqplib' in sys.modules

       self.use_pika          = self.pika_available

   def add_build(self,func):
       self.rebuilds.append(func)

   def add_sleeping(self,func):
       self.sleeping = func
       
   def close(self):
       for channel in self.toclose:
           if self.use_pika : cid = channel.channel_number
           else:              cid = channel.channel_id
           self.logger.debug("closing channel_id: %s" % cid)
           try:    channel.close()
           except: pass
       try:    self.connection.close()
       except: pass
       self.toclose    = []
       self.connection = None

   def connect(self):

       if self.sleeping != None :
          self.asleep = self.sleeping()

       if self.asleep : return

       while True:
          try:
               # connect
               self.logger.debug("Connecting %s %s (ssl %s)" % (self.host,self.user,self.ssl) )
               host = self.host
               if self.port   != None : host = host + ':%s' % self.port
               self.logger.debug("%s://%s:<pw>@%s%s ssl=%s" % (self.protocol,self.user,host,self.vhost,self.ssl))
               if self.use_pika:
                      self.logger.debug("PIKA is used")
                      credentials = pika.PlainCredentials(self.user, self.password)
                      parameters  = pika.connection.ConnectionParameters(self.host,self.port,self.vhost,credentials,ssl=self.ssl)
                      self.connection = pika.BlockingConnection(parameters)
                      logger = logging.getLogger('pika')
                      if self.logger.level != logging.DEBUG : logger.setLevel(logging.CRITICAL)
                      else:                                   logger.setLevel(logging.WARNING)
               else:
                      self.logger.debug("AMQPLIB is used")
                      self.connection = amqp.Connection(host, userid=self.user, password=self.password, \
                                                        virtual_host=self.vhost,ssl=self.ssl)
               self.channel    = self.new_channel()
               self.logger.debug("Connected ")
               for func in self.rebuilds:
                   func()
               break
          except:
               (stype, svalue, tb) = sys.exc_info()
               self.logger.error("AMQP Sender cannot connect to: %s" % self.host)
               self.logger.error("Type=%s, Value=%s" % (stype, svalue))
               if not self.loop : sys.exit(1)
               self.logger.error("Sleeping 5 seconds ...")
               time.sleep(5)

   def exchange_declare(self,exchange,edelete=False,edurable=True):
       try    :
                    self.channel.exchange_declare(exchange, 'topic', auto_delete=edelete,durable=edurable)
                    self.logger.info("declaring exchange %s (%s@%s)" % (exchange,self.user,self.host))
       except :
                    (stype, svalue, tb) = sys.exc_info()
                    self.logger.error("could not declare exchange %s (%s@%s)" % (exchange,self.user,self.host))
                    self.logger.error("Type=%s, Value=%s" % (stype, svalue))

   def exchange_delete(self,exchange):

       # never delete basic and permanent exchanges...

       if exchange in ['xpublic','xreport'] :
          self.logger.info("exchange %s remains" % exchange)
          return

       if exchange.startswith('xwinnow') :
          self.logger.info("exchange %s remains" % exchange)
          return

       # proceed for all others
       try    :
                    self.channel.exchange_delete(exchange)
                    self.logger.info("deleting exchange %s (%s@%s)" % (exchange,self.user,self.host))
       except :
                    (stype, svalue, tb) = sys.exc_info()
                    self.logger.error("could not delete exchange %s (%s@%s)" % (exchange,self.user,self.host))
                    self.logger.error("Type=%s, Value=%s" % (stype, svalue))


   def new_channel(self):
       channel = self.connection.channel()
       self.toclose.append(channel)
       return channel

   def queue_delete(self,queue_name):
       self.logger.info("deleting queue %s (%s@%s)" % (queue_name,self.user,self.host))
       try    :
                    self.channel.queue_delete(queue_name)
       except :
                    (stype, svalue, tb) = sys.exc_info()
                    error_str = '%s' % svalue
                    if 'NOT_FOUND' in error_str : return
                    self.logger.error("could not delete queue %s (%s@%s)" % (queue_name,self.user,self.host))
                    self.logger.error("Type=%s, Value=%s" % (stype, svalue))

   def reconnect(self):
       self.close()
       self.connect()

   def set_credentials(self,protocol,user,password,host,port,vhost):
       self.protocol = protocol
       self.user     = user
       self.password = password
       self.host     = host
       self.port     = port
       self.vhost    = vhost

       if self.protocol == 'amqps' : self.ssl = True
       if self.vhost    == None    : self.vhost = '/'
       if self.vhost    == ''      : self.vhost = '/'

   def set_url(self,url):
       self.protocol = url.scheme
       self.user     = url.username
       self.password = url.password
       self.host     = url.hostname
       self.port     = url.port
       self.vhost    = url.path

       if self.protocol == 'amqps' : 
          self.ssl = True
          if self.port == None :
               self.port=5671

       if self.vhost    == None    : self.vhost = '/'
       if self.vhost    == ''      : self.vhost = '/'

   def set_pika(self,pika=True):

       self.use_pika = pika

       # good choices

       if self.pika_available    and     pika: return
       if self.amqplib_available and not pika: return

       # failback choices

       if self.pika_available    and not pika:
          self.logger.warning("amqplib unavailable : using pika")
          self.use_pika = True
          return

       if self.amqplib_available and     pika:
          self.logger.warning("pika unavailable : using amqplib")
          self.use_pika = False
          return

       # nothing available

       if not self.pika_available and not self.amqplib_available :
          self.logger.error("pika unavailable, amqplib unavailable")
          os._exit(1)


# ==========
# Consumer
# ==========

class Consumer:

   def __init__(self,hostconnect):

      self.hc       = hostconnect
      self.logger   = self.hc.logger
      self.prefetch = 20

      self.exchange_type = 'topic'

      self.hc.add_build(self.build)

      # truncated exponential backoff for consume...
      self.sleep_max  = 1
      self.sleep_min = 0.01
      self.sleep_now = self.sleep_min

      self.for_pika_msg  = None
      if self.hc.use_pika :
         self.for_pika_msg = MY_Message(self.logger)

   def add_prefetch(self,prefetch):
       self.prefetch = prefetch

   def build(self):
       self.logger.debug("building consumer")
       self.channel = self.hc.new_channel()
       if self.prefetch != 0 :
          prefetch_size = 0      # dont care
          a_global      = False  # only apply here
          self.channel.basic_qos(prefetch_size,self.prefetch,a_global)
       
   def ack(self,msg):
       self.logger.debug("--------------> ACK")
       self.logger.debug("--------------> %s" % msg.delivery_tag )
       self.channel.basic_ack(msg.delivery_tag)

   def consume(self,queuename):

       msg = None

       if not self.hc.asleep :
              try :
                      if self.hc.use_pika :
                           self.logger.debug("consume PIKA is used")
                           method_frame, properties, body = self.channel.basic_get(queuename)
                           if method_frame and properties and body :
                              self.for_pika_msg.pika_to_amqplib(method_frame, properties, body )
                              msg = self.for_pika_msg
                      else:
                              self.logger.debug("consume AMQPLIB is used")
                              msg = self.channel.basic_get(queuename)
              except :
                     (stype, value, tb) = sys.exc_info()
                     self.logger.error("consume Type: %s, Value: %s" % (stype, value))
                     self.logger.error("Could not consume in queue %s" % queuename )
                     if self.hc.loop :
                        self.hc.reconnect()
                        self.logger.debug("consume resume ok")
                        if not self.hc.asleep : msg = self.consume(queuename)
       else:
              time.sleep(5)

       # when no message sleep for 1 sec. (value taken from old metpx)
       # *** value 0.01 was tested and would simply raise cpu usage of broker
       # to unacceptable level with very fews processes (~20) trying to consume messages
       # remember that instances and broker sharing messages add up to a lot of consumers

       if msg == None : 
          #self.logger.debug(" no messages received, sleep %5.2fs" % self.sleep_now)
          time.sleep(self.sleep_now)
          self.sleep_now = self.sleep_now * 2
          if self.sleep_now > self.sleep_max : 
                 self.sleep_now = self.sleep_max

       if msg != None :
          self.sleep_now = self.sleep_min 
          #self.logger.debug("--------------> GOT")

       return msg

# ==========
# Publisher
# ==========

class Publisher:

   def __init__(self,hostconnect):
       self.hc     = hostconnect
       self.logger = self.hc.logger
       self.hc.add_build(self.build)

   def build(self):
       self.channel = self.hc.new_channel()
       if self.hc.use_pika :  self.channel.confirm_delivery()
       else:                  self.channel.tx_select()
       
   def publish(self,exchange_name,exchange_key,message,mheaders):
       try :
              if self.hc.use_pika :
                     self.logger.debug("publish PIKA is used")
                     properties = pika.BasicProperties(content_type='text/plain', delivery_mode=1, headers=mheaders)
                     self.channel.basic_publish(exchange_name, exchange_key, message, properties, True )
              else:
                     self.logger.debug("publish AMQPLIB is used")
                     msg = amqp.Message(message, content_type= 'text/plain',application_headers=mheaders)
                     self.channel.basic_publish(msg, exchange_name, exchange_key )
                     self.channel.tx_commit()
              return True
       except :
              if self.hc.loop :
                 (stype, value, tb) = sys.exc_info()
                 self.logger.error("publish Type: %s, Value: %s" % (stype, value))
                 self.logger.error("Sleeping 5 seconds ... and reconnecting")
                 time.sleep(5)
                 self.hc.reconnect()
                 if self.hc.asleep : return False
                 return self.publish(exchange_name,exchange_key,message,mheaders)
              else:
                 (etype, evalue, tb) = sys.exc_info()
                 self.logger.error("publish 2 Type: %s, Value: %s" %  (etype, evalue))
                 self.logger.error("could not publish %s %s %s %s" % (exchange_name,exchange_key,message,mheaders))
                 return False


# ==========
# Queue
# ==========

class Queue:

   def __init__(self,hc,qname,auto_delete=False,durable=False,reset=False):

       self.hc          = hc
       self.logger      = self.hc.logger
       self.name        = qname
       self.qname       = qname
       self.auto_delete = False
       self.durable     = durable
       self.reset       = reset

       self.expire      = 0
       self.message_ttl = 0

       self.bindings    = []

       self.hc.add_build(self.build)

   def add_binding(self,exchange_name,exchange_key):
       self.bindings.append( (exchange_name,exchange_key) )

   def add_expire(self, expire):
       self.expire = expire

   def add_message_ttl(self, message_ttl):
       self.message_ttl = message_ttl

   def bind(self, exchange_name,exchange_key):
       self.channel.queue_bind(self.qname, exchange_name, exchange_key )

   def build(self):
       self.logger.debug("building queue %s" % self.name)
       self.channel = self.hc.new_channel()

       # queue arguments
       args = {}
       if self.expire      > 0 : args['x-expires']     = self.expire
       if self.message_ttl > 0 : args['x-message-ttl'] = self.message_ttl

       # reset 
       if self.reset :
          try    : self.channel.queue_delete( self.name )
          except : self.logger.debug("could not delete queue %s (%s@%s)" % (self.name,self.hc.user,self.hc.host))
                  
       # create queue
       try:
           if self.hc.use_pika:
                  self.logger.debug("queue_declare PIKA is used")
                  q_dclr_ok = self.channel.queue_declare( self.name,
                                       passive=False, durable=self.durable, exclusive=False,
                                       auto_delete=self.auto_delete,
                                       arguments= args )

                  method = q_dclr_ok.method

                  self.qname, msg_count, consumer_count = method.queue, method.message_count, method.consumer_count

           else:
                  self.logger.debug("queue_declare AMQPLIB is used")
                  self.qname, msg_count, consumer_count = \
                      self.channel.queue_declare( self.name,
                                          passive=False, durable=self.durable, exclusive=False,
                                          auto_delete=self.auto_delete,
                                          nowait=False,
                                          arguments= args )
       except : 
              self.logger.error( "queue declare: %s failed...(%s@%s) permission issue ?" % (self.name,self.hc.user,self.hc.host))
              (stype, svalue, tb) = sys.exc_info()
              self.logger.error("build Type: %s, Value: %s" %  (stype, svalue))

       # queue bindings
       for exchange_name,exchange_key in self.bindings:
           self.logger.debug("binding queue to exchange=%s with key=%s" % (exchange_name,exchange_key))
           try:
              self.bind(exchange_name, exchange_key )
           except : 
              self.logger.error( "bind queue: %s to exchange: %s with key: %s failed.." % \
                                 (self.name,exchange_name, exchange_key ) )
              self.logger.error( "Permission issue with %s@%s or exchange %s not found." % \
                                 (self.hc.user,self.hc.host,exchange_name ) )

       self.logger.debug("queue build done")


class sr_consumer:

    def __init__(self, parent, admin=False ):
        self.logger         = parent.logger
        self.logger.debug("sr_consumer __init__")
        self.parent         = parent

        if admin : return

        self.use_pattern    = parent.masks != []
        self.accept_unmatch = parent.accept_unmatch
        self.save = False

        self.build_connection()
        self.build_consumer()
        self.build_queue()
        self.get_message()

    def build_connection(self):
        self.logger.debug("sr_consumer build_broker")

        self.broker     = self.parent.broker

        self.logger.info("AMQP  broker(%s) user(%s) vhost(%s)" % \
                        (self.broker.hostname,self.broker.username,self.broker.path) )

        self.hc = HostConnect( logger = self.logger )
        self.hc.set_pika(self.parent.use_pika)
        self.hc.set_url(self.broker)
        self.hc.connect()

    def build_consumer(self):
        self.logger.debug("sr_consumer build_consumer")

        self.consumer = Consumer(self.hc)

        if self.parent.prefetch > 0 :
            self.consumer.add_prefetch(self.parent.prefetch)

        self.consumer.build()

    def publish_back(self):
        self.logger.debug("sr_consumer publish_back")

        self.publisher = Publisher(self.hc)
        self.publisher.build()

        return self.publisher

    def get_message(self):
        self.logger.debug("sr_consumer get_message")

        if not hasattr(self.parent,'msg'):
           self.parent.msg = sr_message(self.parent)

        self.raw_msg  = None
        self.msg      = self.parent.msg
        self.msg.user = self.broker.username

    def build_queue(self):
        self.logger.debug("sr_consumer build_queue")

        self.broker      = self.parent.broker
        self.bindings    = self.parent.bindings

        self.broker_str  = self.broker.geturl().replace(':'+self.broker.password+'@','@')

        # queue name 
        self.queue_declare(build=False)

        # queue bindings 

        for tup in self.bindings :
            exchange, key = tup
            self.logger.info('Binding queue %s with key %s from exchange %s on broker %s' % \
		            ( self.queue_name, key, exchange, self.broker_str ) )
            self.msg_queue.add_binding( exchange, key )

        # queue creation 
        self.msg_queue.build()

    def close(self):
        self.hc.close()

    def consume(self):

        # acknowledge last message... we are done with it since asking for a new one
        if self.raw_msg != None : self.consumer.ack(self.raw_msg)

        # consume a new one
        self.raw_msg = self.consumer.consume(self.queue_name)
        if self.raw_msg == None : return False, self.msg

        if self.save:
           self.logger.debug("save mode active")

        # make use it as a sr_message

        try :
                 self.msg.from_amqplib(self.raw_msg)
                 self.logger.debug("notice %s " % self.msg.notice)
                 self.logger.debug("urlstr %s " % self.msg.urlstr)
        except :
                 (stype, svalue, tb) = sys.exc_info()
                 self.logger.error("Type: %s, Value: %s,  ..." % (stype, svalue))
                 self.logger.error("malformed message %s"% vars(self.raw_msg))
                 return None, None


        # make use of accept/reject
        if self.use_pattern :

           # Adjust url to account for sundew extension if present, and files do not already include the names.
           if urllib.parse.urlparse(self.msg.urlstr).path.count(":") < 1 and 'sundew_extension' in self.msg.headers.keys() :
              urlstr=self.msg.urlstr + ':' + self.msg.headers[ 'sundew_extension' ]
           else:
              urlstr=self.msg.urlstr

           self.logger.debug("sr_consumer, path being matched: %s " % ( urlstr )  ) 

           if not self.parent.isMatchingPattern(self.msg.urlstr,self.accept_unmatch) :
              self.logger.debug("Rejected by accept/reject options")
              return False,self.msg

        return True,self.msg

    def queue_declare(self,build=False):
        self.logger.debug("sr_consumer queue_declare")

        self.durable     = self.parent.durable
        self.reset       = self.parent.reset
        self.expire      = self.parent.expire
        self.message_ttl = self.parent.message_ttl

        # queue name 
        self.set_queue_name()

        # queue settings
        self.msg_queue   = Queue(self.hc,self.queue_name,durable=self.durable,reset=self.reset)

        if self.expire != None :
           self.msg_queue.add_expire(self.expire)

        if self.message_ttl != None :
           self.msg_queue.add_message_ttl(self.message_ttl)

        # queue creation if needed
        if build :
           self.logger.info("declaring queue %s on %s" % (self.queue_name,self.broker.hostname))
           self.msg_queue.build()

    def random_queue_name(self) :

        # queue file : fix it 

        queuefile  = self.parent.program_name
        if self.parent.config_name :
           queuefile += '.' + self.parent.config_name
        queuefile += '.' + self.broker.username

        # queue path

        self.queuepath = self.parent.user_cache_dir + os.sep + queuefile + '.qname'

        # ====================================================
        # FIXME get rid of this code in 2018 (after release 2.17.11a1)
        # transition old queuepath to new queuepath...

        self.old_queuepath = self.parent.user_cache_dir + os.sep + queuefile
        if os.path.isfile(self.old_queuepath) and not os.path.isfile(self.queuepath) :
           # hardlink (copy of old)
           os.link(self.old_queuepath,self.queuepath)
           # during the transition both should be available is we go back

        # get rid up to the next line
        # ====================================================

        if os.path.isfile(self.queuepath) :
           f = open(self.queuepath)
           self.queue_name = f.read()
           f.close()
           return
        
        self.queue_name  = self.queue_prefix 
        self.queue_name += '.'  + self.parent.program_name

        if self.parent.config_name : self.queue_name += '.'  + self.parent.config_name
        if self.parent.queue_suffix: self.queue_name += '.'  + self.parent.queue_suffix

        self.queue_name += '.'  + str(random.randint(0,100000000)).zfill(8)
        self.queue_name += '.'  + str(random.randint(0,100000000)).zfill(8)

        f = open(self.queuepath,'w')
        f.write(self.queue_name)
        f.close()

    def set_queue_name(self):

        self.broker       = self.parent.broker
        self.queue_prefix = 'q_'+ self.broker.username
        self.queue_name   = self.parent.queue_name

        if self.queue_name :
           if self.queue_prefix in self.queue_name : return
           self.logger.warning("non standard queue name %s" % self.queue_name )
           #self.queue_name = self.queue_prefix + '.'+ self.queue_name
           return

        self.random_queue_name()

    def cleanup(self):
        self.logger.debug("sr_consume cleanup")
        self.build_connection()
        self.set_queue_name()
        self.hc.queue_delete(self.queue_name)
        try    :
                 if hasattr(self,'queuepath') :
                    os.unlink(self.queuepath)
        except : pass

    def declare(self):
        self.logger.debug("sr_consume declare")
        self.build_connection()
        self.queue_declare(build=True)
                  
    def setup(self):
        self.logger.debug("sr_consume setup")
        self.build_connection()
        self.build_queue()


class sr_file():
    def __init__(self, parent) :
        parent.logger.debug("sr_file __init__")

        self.logger      = parent.logger
        self.parent      = parent 

    # cd
    def cd(self, path):
        self.logger.debug("sr_file cd %s" % path)
        os.chdir(path)
        self.path = path

    # chmod
    def chmod(self,perm,path):
        self.logger.debug("sr_file chmod %s %s" % ( "{0:o}".format(perm),path))
        os.chmod(path,perm)

    # close
    def close(self):
        self.logger.debug("sr_file close")
        return

    # connect
    def connect(self):
        self.logger.debug("sr_file connect %s" % self.parent.destination)

        self.recursive   = True
        self.destination = self.parent.destination
        self.timeout     = self.parent.timeout

        self.kbytes_ps = 0
        self.bufsize   = 8192

        if hasattr(self.parent,'kbytes_ps') : self.kbytes_ps = self.parent.kbytes_ps
        if hasattr(self.parent,'bufsize')   : self.bufsize   = self.parent.bufsize

        self.connected   = True

        return True

    # delete
    def delete(self, path):
        self.logger.debug("sr_file rm %s" % path)
        os.unlink(path)

    # ls
    def ls(self):
        self.logger.debug("sr_file ls")
        self.entries  = {}
        self.root = self.path
        self.ls_python(self.path)
        return self.entries

    def ls_python(self,dpath):
        for x in os.listdir(dpath):
            dst = dpath + os.sep + x
            if os.path.isdir(dst):
               if self.recursive : self.ls_python(dst)
               continue
            relpath = dst.replace(self.root,'',1)
            if relpath[0] == '/' : relpath = relpath[1:]

            lstat = os.stat(dst)
            line  = stat.filemode(lstat.st_mode)
            line += ' %d %d %d' % (lstat.st_nlink,lstat.st_uid,lstat.st_gid)
            line += ' %d' % lstat.st_size
            line += ' %s' % time.strftime("%b %d %H:%M", time.localtime(lstat.st_mtime))
            line += ' %s' % relpath
            self.entries[relpath] = line



# file_insert
# called by file_process (general file:// processing)

def file_insert( parent,msg ) :
    parent.logger.debug("file_insert")

    # file must exists
    if not os.path.isfile(msg.relpath):
       fp = open(msg.relpath,'w')
       fp.close()

    fp = open(msg.relpath,'r+b')
    if msg.partflg == 'i' : fp.seek(msg.offset,0)

    ok = file_write_length(fp, msg, parent.bufsize, msg.filesize )

    fp.close()

    return ok


# file_insert_part
# called by file_reassemble : rebuiding file from parts
#
# when inserting, anything that goes wrong means that
# another process is working with this part_file
# so errors are ignored silently 

def file_insert_part(parent,msg,part_file):
    parent.logger.debug("file_insert_part %s" % part_file)
    chk = msg.sumalgo
    try :
             # file disappeared ...
             # probably inserted by another process in parallel
             if not os.path.isfile(part_file):
                parent.logger.debug("file doesnt exist %s" % part_file)
                return False

             # file with wrong size
             # probably being written now by another process in parallel

             lstat    = os.stat(part_file)
             fsiz     = lstat[stat.ST_SIZE] 
             if fsiz != msg.length : 
                parent.logger.debug("file wrong size %s %d %d" % (part_file,fsiz,msg.length))
                return False

             # proceed with insertion

             fp = open(part_file,'rb')
             ft = open(msg.target_file,'r+b')
             ft.seek(msg.offset,0)

             # no worry with length, read all of part_file
             # compute onfly_checksum ...

             bufsize = parent.bufsize
             if bufsize > msg.length : bufsize = msg.length

             if chk : chk.set_path(os.path.basename(msg.target_file))

             i  = 0
             while i<msg.length :
                   buf = fp.read(bufsize)
                   if not buf: break
                   ft.write(buf)
                   if chk : chk.update(buf)
                   i  += len(buf)

             if ft.tell() >= msg.filesize:
                 ft.truncate()

             ft.close() 
             fp.close()

             if i != msg.length :
                msg.logger.error("file_insert_part file currupted %s" % part_file)
                msg.logger.error("read up to  %d of %d " % (i,msg.length) )
                lstat   = os.stat(part_file)
                fsiz    = lstat[stat.ST_SIZE] 
                msg.logger.error("part filesize  %d " % (fsiz) )

             # set checksum in msg
             if chk : msg.onfly_checksum = chk.get_value()

             # remove inserted part file

             try    : os.unlink(part_file)
             except : pass

             # run on part... if provided

             if parent.on_part :
                ok = parent.on_part(parent)
                if not ok : 
                   msg.logger.warning("inserted but rejected by on_part %s " % part_file)
                   msg.logger.warning("the file may not be correctly reassemble %s " % msg.target_file)
                   return ok

    # oops something went wrong

    except :
             (stype, svalue, tb) = sys.exc_info()
             msg.logger.debug("Type: %s, Value: %s,  ..." % (stype, svalue))
             msg.logger.debug("did not insert %s " % part_file)
             return False

    # success: log insertion

    msg.report_publish(201,'Inserted')

    # publish now, if needed, that it is inserted

    if msg.publisher : 
       msg.set_topic('v02.post',msg.target_relpath)
       msg.set_notice(msg.new_baseurl,msg.target_relpath,msg.time)
       if chk :
          if    msg.sumflg == 'z' :
                msg.set_sum(msg.checksum,msg.onfly_checksum)
          else: msg.set_sum(msg.sumflg,  msg.onfly_checksum)

       parent.__on_post__()
       msg.report_publish(201,'Publish')

    # if lastchunk, check if file needs to be truncated
    file_truncate(parent,msg)

    # ok we reassembled the file and it is the last chunk... call on_file
    if msg.lastchunk : 
       msg.logger.warning("file assumed complete with last part %s" % msg.target_file)
       #if parent.on_file:
       #   ok = parent.on_file(parent)
       for plugin in parent.on_file_list:
          ok = plugin(parent)
          if not ok: return False

    return True


# file_link
# called by file_process (general file:// processing)

def file_link( msg ) :

    try    : os.unlink(msg.new_file)
    except : pass
    try    : os.link(msg.relpath,msg.new_file)
    except : return False

    msg.compute_local_checksum()
    msg.onfly_checksum = msg.local_checksum

    msg.report_publish( 201, 'Linked')

    return True

# file_process (general file:// processing)

def file_process( parent ) :
    parent.logger.debug("file_process")

    msg = parent.msg

    try:    curdir = os.getcwd()
    except: curdir = None

    if curdir != parent.new_dir:
       os.chdir(parent.new_dir)

    # try link if no inserts

    if msg.partflg == '1' or \
       (msg.partflg == 'p' and  msg.in_partfile) :
       ok = file_link(msg)
       if ok :
          if parent.delete :
              try: 
                  os.unlink(msg.relpath)
              except: 
                  msg.logger.error("delete of link to %s failed"%(msg.relpath))
          return ok

    # This part is for 2 reasons : insert part
    # or copy file if preceeding link did not work
    try :
             ok = file_insert(parent,msg)
             if parent.delete :
                if msg.partflg.startswith('i'):
                   msg.logger.info("delete unimplemented for in-place part files %s" %(msg.relpath))
                else:
                   try: 
                       os.unlink(msg.relpath)
                   except: 
                       msg.logger.error("delete of %s after copy failed"%(msg.relpath))

             if ok : return ok

    except : 
             (stype, svalue, tb) = sys.exc_info()
             msg.logger.debug("Type: %s, Value: %s,  ..." % (stype, svalue))

    msg.report_publish(499,'Not Copied')
    msg.logger.error("could not copy %s in %s"%(msg.relpath,msg.new_file))

    return False

# file_reassemble : rebuiding file from parts
# when ever a part file is processed (inserted or written in part_file)
# this module is called to try inserting any part_file left

def file_reassemble(parent):
    parent.logger.debug("file_reassemble")

    msg = parent.msg

    if not hasattr(msg,'target_file') or msg.target_file == None : return

    try:    curdir = os.getcwd()
    except: curdir = None

    if curdir != parent.new_dir:
       os.chdir(parent.new_dir)

    # target file does not exit yet

    if not os.path.isfile(msg.target_file) :
       msg.logger.debug("insert_from_parts: target_file not found %s" % msg.target_file)
       return

    # check target file size and pick starting part from that

    lstat   = os.stat(msg.target_file)
    fsiz    = lstat[stat.ST_SIZE] 
    i       = int(fsiz /msg.chunksize)

    msg.logger.debug("verify ingestion : block = %d of %d" % (i,msg.block_count))
       
    while i < msg.block_count:

          # setting block i in message

          msg.current_block = i
          msg.set_parts('i',msg.chunksize,msg.block_count,msg.remainder,msg.current_block)
          msg.set_suffix()

          # set part file

          part_file = msg.target_file + msg.suffix
          if not os.path.isfile(part_file) :
             msg.logger.debug("part file %s not found, stop insertion" % part_file)
             # break and not return because we want to check the lastchunk processing
             break

          # check for insertion (size may have changed)

          lstat   = os.stat(msg.target_file)
          fsiz    = lstat[stat.ST_SIZE] 
          if msg.offset > fsiz :
             msg.logger.debug("part file %s no ready for insertion (fsiz %d, offset %d)" % (part_file,fsiz,msg.offset))
             break


          # insertion attempt... should work unless there is some race condition

          ok = file_insert_part(parent,msg,part_file)
          # break and not return because we want to check the lastchunk processing
          if not ok : break
          i = i + 1

    # if lastchunk, check if file needs to be truncated
    file_truncate(parent,msg)



# file_write_length
# called by file_process->file_insert (general file:// processing)

def file_write_length(req,msg,bufsize,filesize):
    msg.logger.debug("file_write_length")

    msg.onfly_checksum = None

    chk = msg.sumalgo
    msg.logger.debug("file_write_length chk = %s" % chk)
    if chk : chk.set_path(msg.new_file)

    # file should exists
    if not os.path.isfile(msg.new_file) :
       fp = open(msg.new_file,'w')
       fp.close()

    # file open read/modify binary
    fp = open(msg.new_file,'r+b')
    if msg.local_offset != 0 : fp.seek(msg.local_offset,0)

    nc = int(msg.length/bufsize)
    r  =     msg.length%bufsize

    # read/write bufsize "nc" times
    i  = 0
    while i < nc :
          chunk = req.read(bufsize)
          fp.write(chunk)
          if chk : chk.update(chunk)
          i = i + 1

    # remaining
    if r > 0 :
       chunk = req.read(r)
       fp.write(chunk)
       if chk : chk.update(chunk)

    if fp.tell() >= msg.filesize:
       fp.truncate()

    fp.close()
  
    h = self.parent.msg.headers
    if self.parent.preserve_mode and 'mode' in h :
       try   : mod = int( h['mode'], base=8)
       except: mod = 0
       if mod > 0 : os.chmod(msg.new_file, mod )

    if self.parent.preserve_time and 'mtime' in h and h['mtime'] :
        os.utime(msg.new_file, times=( timestr2flt( h['atime']), timestr2flt( h[ 'mtime' ] )))

    if chk : msg.onfly_checksum = chk.get_value()

    msg.report_publish(201,'Copied')

    return True

# file_truncate
# called under file_reassemble (itself and its file_insert_part)
# when inserting lastchunk, file may need to be truncated

def file_truncate(parent,msg):

    # will do this when processing the last chunk
    # whenever that is
    if not msg.lastchunk : return

    try :
             lstat   = os.stat(msg.target_file)
             fsiz    = lstat[stat.ST_SIZE] 

             if fsiz > msg.filesize :
                fp = open(msg.target_file,'r+b')
                fp.truncate(msg.filesize)
                fp.close()

                msg.set_topic('v02.post',msg.target_relpath)
                msg.set_notice(msg.new_baseurl,msg.target_relpath,msg.time)
                msg.report_publish(205, 'Reset Content :truncated')

    except : pass


amqp_ss_maxlen = 253

class sr_message():

    def __init__(self,logger):
        self.logger        = logger

        self.bufsize       = 8192

        self.exchange      = None
        self.report_exchange  = 'xs_anonymous'
        self.report_publisher = None
        self.publisher     = None
        self.pub_exchange  = None
        self.topic         = None
        self.notice        = None
        self.headers       = {}

        self.partstr       = None
        self.sumstr        = None
        self.sumflg        = None

        self.part_ext      = 'Part'

        self.sumalgo       = checksum_d()
        self.lastflg       = ''

        self.inplace       = True
        self.report_back   = False

        self.user          = None

        self.host          = socket.getfqdn()

    def log_error(self):
        self.report_publish(self.code,self.message)

    def change_partflg(self, partflg ):
        self.partflg       =  partflg 
        self.partstr       = '%s,%d,%d,%d,%d' %\
                             (partflg,self.chunksize,self.block_count,self.remainder,self.current_block)

    def content_should_not_be_downloaded(self):
        """
        if the file advertised is newer than the local one, and it has a different checksum, return True.

        """
        self.logger.debug("sr_message content_match")
        self.local_checksum = None

        if not os.path.isfile(self.new_file) : return False

        # insert : file big enough to compute part checksum ?

        lstat = os.stat(self.new_file)
        fsiz  = lstat[stat.ST_SIZE] 
        end   = self.local_offset + self.length

        # compare dates...
        if self.sumflg in ['0','n','z'] : 
            return False
 
        if end > fsiz :
           self.logger.warning("sr_message content_match file not big enough (insert?)")
           return False

        self.compute_local_checksum()

        return self.local_checksum == self.checksum

    def compute_local_checksum(self):
        self.logger.debug("sr_message compute_local_checksum")

        bufsize = self.bufsize
        if self.length < bufsize : bufsize = self.length

        self.sumalgo.set_path(os.path.basename(self.new_file))

        fp = open(self.new_file,'rb')
        if self.local_offset != 0 : fp.seek(self.local_offset,0)
        i  = 0
        while i<self.length :
              buf = fp.read(bufsize)
              if not buf: break
              self.sumalgo.update(buf)
              i  += len(buf)
        fp.close()

        if i != self.length :
           self.logger.warning("sr_message compute_local_checksum incomplete reading %d %d" % (i,self.length))
           self.local_checksum = '0'
           return

        self.local_checksum = self.sumalgo.get_value()


    def from_amqplib(self, msg=None ):

        self.start_timer()

        #self.logger.debug("attributes= %s" % vars(msg))
        if msg :
           self.exchange  = msg.delivery_info['exchange']
           self.topic     = msg.delivery_info['routing_key']
           self.headers   = msg.properties['application_headers']
           self.notice    = msg.body

           if type(msg.body) == bytes: self.notice = msg.body.decode("utf-8")

        # retransmission case :
        # topic is name of the queue...
        # set exchange to xpublic
        # rebuild topic from notice : v02.post....

        if self.exchange == '' and self.topic[:2] == 'q_':
           self.logger.debug(" retransmit topic = %s" % self.topic)
           token = self.notice.split(' ')
           self.exchange = 'xpublic'
           if hasattr(self.headers,'exchange') :
              self.exchange = self.headers['exchange']
              del self.headers['exchange']

           path  = token[2].strip('/')
           words = path.split('/')
           self.topic    = 'v02.post.' + '.'.join(words[:-1])
           self.logger.debug(" modified for topic = %s" % self.topic)

        # adjust headers from -headers option

        #self.trim_headers()

        self.partstr     = None
        self.sumstr      = None

        token        = self.topic.split('.')
        self.version = token[0]

        if self.version == 'v00' :
           self.parse_v00_post()

        if self.version == 'v02' :
           self.parse_v02_post()

    def get_elapse(self):
        return time.time()-self.tbegin

    def report_publish(self,code,message):
        self.code               = code
        self.headers['message'] = message
        self.report_topic          = self.topic.replace('.post.','.report.')
        self.report_notice         = "%s %d %s %s %f" % \
                                  (self.notice,self.code,self.host,self.user,self.get_elapse())
        self.set_hdrstr()

        # AMQP limits topic to 255 characters, so truncate and warn.
        if len(self.topic.encode("utf8")) >= amqp_ss_maxlen :
           mxlen=amqp_ss_maxlen 
           # see truncation explanation from above.
           while( self.report_topic.encode("utf8")[mxlen-1] & 0xc0 == 0xc0 ):
               mxlen -= 1

           self.report_topic = self.report_topic.encode("utf8")[0:mxlen].decode("utf8")
           self.logger.warning( "truncating reporting topic at %d characters (to fit 255 byte AMQP limit) to: %s " % \
                        ( len(self.report_topic) , self.report_topic ) )

        if self.report_back and self.report_publisher != None :
           self.report_publisher.publish(self.report_exchange,self.report_topic,self.report_notice,self.headers)

        self.logger.debug("%d %s : %s %s %s" % (code,message,self.report_topic,self.report_notice,self.hdrstr))

        # make sure not published again
        del self.headers['message']

    def parse_v00_post(self):
        token             = self.topic.split('.')
        # v00             = token[0]
        # dd              = token[1]
        # notify          = token[2]
        self.version      = 'v02'
        self.mtype        = 'post'
        self.topic_prefix = 'v02.post'
        self.subtopic     = '.'.join(token[3:])
        self.topic        = self.topic_prefix + '.' + self.subtopic

        token        = self.notice.split(' ')
        self.baseurl = token[2]
        self.relpath = token[3].replace('%20',' ')

        self.set_notice(token[2],token[3])

        url          = urllib.parse.urlparse(token[2]+token[3])
        
        self.checksum = token[0]
        self.filesize = int(token[1])

        self.headers['source'] = 'metpx'

        self.partstr = '1,%d,1,0,0' % self.filesize
        self.headers['parts'] = self.partstr

        self.sumstr  = 'd,%s' % self.checksum
        self.headers['sum'] = self.sumstr

        self.to_clusters = []
        self.headers['to_clusters'] = None

        self.suffix  = ''
        
        self.set_parts_str(self.partstr)
        self.set_sum_str(self.sumstr)
        self.set_suffix()
        self.set_hdrstr()

    def parse_v02_post(self):

        token         = self.topic.split('.')
        self.version  = token[0]
        self.mtype    = token[1]
        self.topic_prefix = '.'.join(token[:2])
        self.subtopic     = '.'.join(token[3:])

        token        = self.notice.split(' ')
        self.time    = token[0]
        self.baseurl = token[1]
        self.relpath = token[2].replace('%20',' ')
        self.urlstr  = token[1]+token[2]
        #self.url     = urllib.parse.urlparse(self.urlstr)

        if self.mtype == 'report' or self.mtype == 'log': # log included for compatibility... prior to rename..
           self.report_code   = int(token[3])
           self.report_host   = token[4]
           self.report_user   = token[5]
           self.report_elapse = float(token[6])

        self.partstr = None
        if 'parts'   in self.headers :
           self.partstr  = self.headers['parts']

        self.sumstr  = None
        if 'sum'     in self.headers :
           self.sumstr   = self.headers['sum']

        self.to_clusters = []
        if 'to_clusters' in self.headers :
           self.to_clusters  = self.headers['to_clusters'].split(',')

        self.suffix = ''

        self.set_parts_str(self.partstr)
        self.set_sum_str(self.sumstr)
        self.set_suffix()
        self.set_msg_time()
        self.set_hdrstr()

    def part_suffix(self):
        return '.%d.%d.%d.%d.%s.%s' %\
               (self.chunksize,self.block_count,self.remainder,self.current_block,self.sumflg,self.part_ext)

    def publish(self):
        ok = False

        if self.pub_exchange != None : self.exchange = self.pub_exchange

        for h in self.headers:
           if len(self.headers[h].encode("utf8")) >= amqp_ss_maxlen:

                # strings in utf, and if names have special characters, the length
                # of the encoded string wll be longer than what is returned by len(. so actually need to look
                # at the encoded length ...  len ( self.headers[h].encode("utf-8") ) < 255
                # but then how to truncate properly. need to avoid invalid encodings.
                mxlen=amqp_ss_maxlen
                while( self.headers[h].encode("utf8")[mxlen-1] & 0xc0 == 0xc0 ):
                      mxlen -= 1

                self.headers[h] = self.headers[h].encode("utf8")[0:mxlen].decode("utf8")
                self.logger.warning( "truncating %s header at %d characters (to fit 255 byte AMQP limit) to: %s " % \
                        ( h, len(self.headers[h]) , self.headers[h]) )
                
        # AMQP limits topic to 255 characters, so truncate and warn.
        if len(self.topic.encode("utf8")) >= amqp_ss_maxlen :
           mxlen=amqp_ss_maxlen 
           # see truncation explanation from above.
           while( self.topic.encode("utf8")[mxlen-1] & 0xc0 == 0xc0 ):
               mxlen -= 1

           self.topic = self.topic.encode("utf8")[0:mxlen].decode("utf8")
           self.logger.warning( "truncating topic at %d characters (to fit 255 byte AMQP limit) to: %s " % \
                        ( len(self.topic) , self.topic ) )
        

        # in order to split winnowing into multiple instances, directs items with same checksum
        # to same shard. do that by keying on the last character of the checksum.
        # 
        if self.post_exchange_split > 0 :
           suffix= "%02d" % ( ord(self.sumstr[-1]) % self.post_exchange_split )
           self.logger.debug( "post_exchange_split set, keying on %s , suffix is %s" % ( self.sumstr[-1], suffix) )
        else:
           suffix=""

        if self.publisher != None :
           ok = self.publisher.publish(self.exchange+suffix,self.topic,self.notice,self.headers)

        self.set_hdrstr()

        if ok :
                self.logger.debug("Published1: %s %s" % (self.exchange,self.topic))
                self.logger.debug("Published2: '%s' %s" % (self.notice, self.hdrstr))
        else  :
                self.printlog = self.logger.error
                self.printlog("Could not publish message :")

                self.printlog("exchange %s topic %s " % (self.exchange,self.topic))
                self.printlog("notice   %s"           % self.notice )
                self.printlog("headers  %s"           % self.hdrstr )

        return ok

    def set_exchange(self,name):
        self.exchange = name

    def set_file(self, new_file, sumstr):
        """ 
            set_file: modify a message to reflect a new file.
                      make a file URL of the new_file.
            sumstr should be the properly formatted checksum field for a message
              '<algorithm>,<value>', e.g.  'd,cbe9047d1b979561bed9a334111878c6'
            to be used by filter plugins when changing the output url.
        """
        fstat = os.stat(new_file)

        # Modify message for posting.

        self.baseurl = 'file:'
        self.relpath = new_file

        self.urlstr = 'file:/' + new_file
        self.url = urllib.parse.urlparse(self.urlstr)

        path  = new_file.strip('/')
        words = path.split('/')
        self.topic = 'v02.post.' + '.'.join(words[:-1])

        self.headers[ 'sum' ] = sumstr
        self.headers[ 'parts' ] = '1,%d,0,0' % fstat.st_size
        self.headers[ 'mtime' ] = timeflt2str(fstat.st_mtime)

        self.set_notice(self.baseurl,self.relpath)

    def set_hdrstr(self):
        self.hdrstr  = ''

        for h in self.headers:
           self.hdrstr += '%s=%s ' % (h, self.headers[h])

        # added for v00 compatibility (old version of dd_subscribe)
        # can be taken off when v02 will be fully deployed and end user uses sr_subscribe
        #self.headers['filename'] = os.path.basename(self.relpath).split(':')[0][0:200]


    # Once we know the local file we want to use
    # we can have a few flavor of it

    def set_new(self,inplace,opath):

        self.local_offset  = 0
        self.in_partfile   = False
        self.local_checksum= None
       
        self.inplace       = self.inplace
        self.new_dir       = os.path.dirname(opath)
        self.new_file      = os.path.basename(opath)
        self.new_baseurl   = 'file:'
        self.new_relpath   = opath

        self.target_file   = None

        # file to file

        if self.partflg == '1' : return
        if self.partflg == None : return
     
        # part file never inserted

        if not self.inplace :

           self.in_partfile = True

           # part file to part file

           if self.partflg == 'p' : return

           # file inserts to part file

           if self.partflg == 'i' :
              self.new_file    += self.suffix
              self.new_relpath += self.suffix
              return

        
        # part file inserted

        if self.inplace :

           # part file inserts to file (maybe in file, maybe in part file)

           if self.partflg == 'p' :
              self.target_file    = self.new_file.replace(self.suffix,'')
              self.target_relpath = self.new_relpath.replace(self.suffix,'')
              part_file    = self.new_file
              part_relpath = self.new_relpath

        
           # file insert inserts into file (maybe in file, maybe in part file)

           if self.partflg == 'i' :
              self.target_file    = self.new_file
              self.target_relpath = self.new_relpath
              part_file           = self.new_file + self.suffix
              part_relpath        = self.new_relpath + self.suffix

           # default setting : redirect to temporary part file

           self.new_file    = part_file
           self.new_relpath = part_relpath
           self.in_partfile = True
        
           # try to make this message a file insert

           # file exists
           self.target_path = self.new_dir + os.sep + self.target_file
           if os.path.isfile(self.target_path) :
              self.logger.debug("new_file exists")
              lstat   = os.stat(self.target_path)
              fsiz    = lstat[stat.ST_SIZE] 

              self.logger.debug("offset vs fsiz %d %d" % (self.offset,fsiz ))
              # part/insert can be inserted 
              if self.offset <= fsiz :
                 self.logger.debug("insert")
                 self.new_file     = self.target_file
                 self.new_relpath  = self.target_relpath
                 self.local_offset = self.offset
                 self.in_partfile  = False
                 return

              # in temporary part file
              self.logger.debug("exist but no insert")
              return


           # file does not exists but first part/insert ... write directly to new_file
           elif self.current_block == 0 :
              self.logger.debug("not exist but first block")
              self.new_file    = self.target_file
              self.new_relpath = self.target_relpath
              self.in_partfile = False
              return

           # file does not exists any other part/insert ... put in temporary part_file
           else :
              self.logger.debug("not exist and not first block")
              self.in_partfile = True
              return
                 
        # unknow conditions

        self.logger.error("bad unknown conditions")
        return

    def set_msg_time(self):
        parts       = self.time.split('.')
        ts          = time.strptime(parts[0], "%Y%m%d%H%M%S" )
        ep_msg      = calendar.timegm(ts)
        self.tbegin = ep_msg + int(parts[1]) / 1000.0

    def set_notice_url(self,url,time=None):
        self.url    = url
        self.time   = time
        if time    == None : self.set_time()
        path        = url.path.strip('/')
        notice_path = path.replace(' ','%20')

        if url.scheme == 'file' :
           self.notice = '%s %s %s' % (self.time,'file:','/'+notice_path)
           return

        urlstr      = url.geturl()
        static_part = urlstr.replace(url.path,'') + '/'

        if url.scheme == 'http' :
           self.notice = '%s %s %s' % (self.time,static_part,notice_path)
           return

        if url.scheme[-3:] == 'ftp'  :
           if url.path[:2] == '//'   : notice_path = '/' + notice_path

        self.notice = '%s %s %s' % (self.time,static_part,notice_path)

    def set_notice(self,baseurl,relpath,time=None):

        self.time    = time
        self.baseurl = baseurl
        self.relpath = relpath
        if not time  : self.set_time()

        notice_relpath = relpath.replace(' ','%20')

        self.notice = '%s %s %s' % (self.time,baseurl,notice_relpath)

        #========================================
        # COMPATIBILITY TRICK  for the moment

        self.urlstr  = baseurl+notice_relpath
        self.url     = urllib.parse.urlparse(self.urlstr)
        #========================================


    def set_parts(self,partflg='1',chunksize=0, block_count=1, remainder=0, current_block=0):
        self.partflg          = partflg 
        self.chunksize        = chunksize
        self.block_count      = block_count
        self.remainder        = remainder
        self.current_block    = current_block
        self.partstr          = '%s,%d,%d,%d,%d' %\
                                (partflg,chunksize,block_count,remainder,current_block)
        self.lastchunk        = current_block == block_count-1
        self.headers['parts'] = self.partstr

        self.offset        = self.current_block * self.chunksize
        self.filesize      = self.block_count * self.chunksize
        if self.remainder  > 0 :
           self.filesize  += self.remainder   - self.chunksize
           if self.lastchunk : self.length    = self.remainder

    def set_parts_str(self,partstr):

        self.partflg = None
        self.partstr = partstr

        if self.partstr == None : return

        token        = self.partstr.split(',')
        self.partflg = token[0]

        self.chunksize     = int(token[1])
        self.block_count   = 1
        self.remainder     = 0
        self.current_block = 0
        self.lastchunk     = True

        self.offset        = 0
        self.length        = self.chunksize

        self.filesize      = self.chunksize

        if self.partflg in [ '0', '1' ]: return

        self.block_count   = int(token[2])
        self.remainder     = int(token[3])
        self.current_block = int(token[4])
        self.lastchunk     = self.current_block == self.block_count-1

        self.offset        = self.current_block * self.chunksize

        self.filesize      = self.block_count * self.chunksize

        if self.remainder  > 0 :
           self.filesize  += self.remainder   - self.chunksize
           if self.lastchunk : self.length    = self.remainder

    def set_rename(self,rename=None):
        if rename != None :
           self.headers['rename'] = rename
        elif 'rename' in self.headers :
           del self.headers['rename']

    def set_source(self,source=None):
        if source != None :
           self.headers['source'] = source
        elif 'source' in self.headers :
           del self.headers['source']

    def set_sum(self,sumflg='d',checksum=0):
        self.sumflg   =  sumflg
        self.checksum =  checksum
        self.sumstr   = '%s,%s' % (sumflg,checksum)
        self.headers['sum'] = self.sumstr

    def set_sumalgo(self,sumflg):

        if sumflg == self.lastflg : return

        flgs = sumflg

        if len(sumflg) > 2 and sumflg[:2] in ['z,','M,']:
           flgs = sumflg[2:]

        if flgs == self.lastflg : return
        self.lastflg = flgs

        if flgs == 'd' :
           self.sumalgo = checksum_d()
           return

        if flgs == 'n' :
           self.sumalgo = checksum_n()
           return

        if flgs in [ 's' ]:
           self.sumalgo = checksum_s()
           return

        if flgs in [ '0', 'L', 'R' ]:
           self.sumalgo = checksum_0()
           return

        if self.sumalgo == None : sum_error = True

        if not sum_error and not hasattr(self.sumalgo,'set_path' ) : sum_error = True
        if not sum_error and not hasattr(self.sumalgo,'update'   ) : sum_error = True
        if not sum_error and not hasattr(self.sumalgo,'get_value') : sum_error = True

        if sum_error :
           self.logger.error("sumflg %s not working... set to 'd'" % sumflg)
           self.lastflg = 'd'
           self.sumalgo = checksum_d()


    def set_sum_str(self,sumstr):
        self.sumflg  = None
        self.sumalgo = None
        self.sumstr  = sumstr
        if sumstr == None : return

        token        = self.sumstr.split(',')
        self.sumflg  = token[0]
        self.checksum= token[1]

        # file to be removed
        if self.sumflg == 'R' : return

        # keep complete z description in sumflg
        if self.sumflg == 'z':
           self.sumflg  = sumstr

        self.set_sumalgo(self.sumflg)

    def set_suffix(self):
        if self.partstr == None : return
        if self.sumstr  == None or self.sumflg == 'R' : return
        self.suffix = self.part_suffix()

    def set_time(self):
        now  = time.time()
        msec = '.%d' % (int(round(now * 1000)) %1000)
        nows = time.strftime("%Y%m%d%H%M%S",time.gmtime()) + msec
        self.time = nows
        if not hasattr(self,'tbegin') : self.tbegin = now

    def set_to_clusters(self,to_clusters=None):
        if to_clusters != None :
           self.headers['to_clusters'] = to_clusters
           self.to_clusters = to_clusters.split(',')
        elif 'to_clusters' in self.headers :
           del self.headers['to_clusters']
           self.to_clusters = []

    def set_topic(self,topic_prefix,relpath):
        self.topic_prefix = topic_prefix
        self.topic        = topic_prefix
        self.subtopic     = ''

        strpath           = relpath.strip('/')
        words             = strpath.split('/')
        if len(words) > 1 :
           self.subtopic = '.'.join(words[:-1])
           self.topic   += '.' + self.subtopic

        self.topic        = self.topic.replace('..','.')

    def set_topic_url(self,topic_prefix,url):
        self.topic_prefix = topic_prefix
        self.topic        = topic_prefix
        self.subtopic     = ''
        relpath           = url.path

        # MG compat ?
        self.url          = url

        strpath           = relpath.strip('/')
        words             = strpath.split('/')
        if len(words) > 1 :
           self.subtopic = '.'.join(words[:-1])
           self.topic   += '.' + self.subtopic

        self.topic        = self.topic.replace('..','.')
        self.logger.debug("set_topic_url topic %s" % self.topic )
       

    def set_topic_usr(self,topic_prefix,subtopic):
        self.topic_prefix = topic_prefix
        self.subtopic     = subtopic
        self.topic        = topic_prefix + '.' + self.subtopic
        self.topic        = self.topic.replace('..','.')


    def start_timer(self):
        self.tbegin = time.time()


    # adjust headers from -headers option

    def trim_headers(self):
        self.logger.debug("trim_headers")

        for k in self.del_headers:
            if k in self.headers : del self.headers[k]

        for k in self.add_headers:
            if k in self.headers : continue
            self.headers[k] = self.add_headers[k]

    def verify_part_suffix(self,filepath):
        filename = os.path.basename(filepath)
        token    = filename.split('.')

        try :  
                 self.suffix = '.' + '.'.join(token[-6:])
                 if token[-1] != self.part_ext :
                    return False,'not right extension',None,None,None

                 self.chunksize     = int(token[-6])
                 self.block_count   = int(token[-5])
                 self.remainder     = int(token[-4])
                 self.current_block = int(token[-3])
                 self.sumflg        = token[-2]

                 if self.current_block >= self.block_count :
                    return False,'current block wrong',None,None,None
                 if self.remainder     >= self.chunksize   :
                    return False,'remainder too big',None,None,None

                 self.length    = self.chunksize
                 self.lastchunk = self.current_block == self.block_count-1
                 self.filesize  = self.block_count * self.chunksize
                 if self.remainder  > 0 :
                    self.filesize  += self.remainder - self.chunksize
                    if self.lastchunk : self.length  = self.remainder

                 lstat     = os.stat(filepath)
                 fsiz      = lstat[stat.ST_SIZE] 

                 if fsiz  != self.length :
                    return False,'wrong file size',None,None,None

                 # compute chksum
                 self.set_sumalgo(self.sumflg)

                 self.sumalgo.set_path(filepath)
                 fp = open(filepath,'rb')
                 i  = 0
                 while i<fsiz :
                       buf = fp.read(self.bufsize)
                       if not buf : break
                       self.sumalgo.update(buf)
                       i  += len(buf)
                 fp.close()

                 if i != fsiz :
                    self.logger.warning("sr_message verify_part_suffix incomplete reading %d %d" % (i,fsiz))
                    return False,'missing data from file', None,None,None

                 # set chksum
                 self.checksum  = self.sumalgo.get_value()


                 # set partstr
                 self.partstr = 'p,%d,%d,%d,%d' %\
                   (self.chunksize,self.block_count,self.remainder,self.current_block)

                 # set sumstr
                 self.sumstr  = '%s,%s' % (self.sumflg,self.checksum)

        except :
                 (stype, svalue, tb) = sys.exc_info()
                 self.logger.error("Type: %s, Value: %s" % (stype, svalue))
                 return False,'incorrect extension',None,None,None

        return True,'ok',self.suffix,self.partstr,self.sumstr

from hashlib import sha512

# ===================================
# checksum_0 class
# ===================================

class checksum_0(object):
      """
      Trivial minimalist checksumming algorithm, returns 0 for any file.
      """
      def __init__(self):
          self.value = '0'

      def get_value(self):
          return self.value

      def update(self,chunk):
          pass

      def set_path(self,path):
          pass

# ===================================
# checksum_d class
# ===================================

class checksum_d(object):
      """
      The default algorithm is to do a checksum of the entire contents of the file, which is called 'd'.
      """
      def __init__(self):
          self.value = '0'

      def get_value(self):
          self.value = self.filehash.hexdigest()
          return self.value

      def update(self,chunk):
          self.filehash.update(chunk)

      def set_path(self,path):
          self.filehash = md5()

# ===================================
# checksum_s class
# ===================================

class checksum_s(object):
      """
      The SHA512 algorithm to checksum the entire file, which is called 's'.
      """
      def __init__(self):
          self.value = '0'

      def get_value(self):
          self.value = self.filehash.hexdigest()
          return self.value

      def update(self,chunk):
          self.filehash.update(chunk)

      def set_path(self,path):
          self.filehash = sha512()

# ===================================
# checksum_n class
# ===================================

class checksum_n(object):
      """
      when there is more than one processing chain producing products, it can happen that files are equivalent
      without being identical, for example if each server tags a product with ''generated on server 16', then
      the generation tags will differ.   The simplest option for checksumming then is to use the name of the
      product, which is generally the same from all the processing chains.  
      """
      def __init__(self):
          self.value = '0'

      def get_value(self):
          return self.value

      def update(self,chunk):
          pass

      def set_path(self,path):
          filename   = os.path.basename(path)
          self.value = md5(bytes(filename,'utf-8')).hexdigest()

def timeflt2str( f ):
    msec = '.%d' % ((f%1)*1000)
    s  = time.strftime("%Y%m%d%H%M%S",time.gmtime(f)) + msec
    return(s) 
    

def timestr2flt( s ):
    t=datetime.datetime(  int(s[0:4]), int(s[4:6]), int(s[6:8]), int(s[8:10]), int(s[10:12]), int(s[12:14]), 0, datetime.timezone.utc )
    f=calendar.timegm(  t.timetuple())+float('0'+s[14:])
    return(f)

class ConsumerX(object):

    def __init__(self,config,logger):
        self.pgm        = os.path.basename(sys.argv[0])
        self.logger     = logger

        self.connected  = False

        self.connection  = None
        self.channel     = None
        self.log_channel = None
        self.ssl        = False

        self.queue      = None
        self.durable    = True
        self.expire     = None

        self.notify_only = False
        self.discard = False
        
        self.config = config
        self.name   = config

        self.amqp_log = None
        self.myinit()

    def close(self):
       self.hc.close()
       self.connected = False

    def connect(self):

        self.hc = None

        self.hc = HostConnect( logger = self.logger )
        self.hc.set_url(self.broker)
        self.hc.connect()

        self.consumer = Consumer(self.hc)
        self.consumer.add_prefetch(1)
        self.consumer.build()

        #should not declare exchange just use them
        #ex = Exchange(self.hc,self.exchange)
        #ex.build()

        self.msg_queue = Queue(self.hc,self.queue,durable=self.durable)
        if self.expire != None :
           self.msg_queue.add_expire(self.expire)
        if self.message_ttl != None :
           self.msg_queue.add_message_ttl(self.message_ttl)


        if self.ssl:
           sproto='amqps'
        else:
           sproto='amqp'

        for k in self.exchange_key :
           self.logger.info('Binding queue %s with key %s to exchange %s on broker %s://%s@%s%s', 
		self.queue, k, self.exchange, self.broker.scheme, self.broker.username, self.broker.hostname,self.broker.path )
           self.msg_queue.add_binding(self.exchange, k )

        self.msg_queue.build()

        if self.log_back :
           self.amqp_log = Publisher(self.hc)
           self.amqp_log.build()

    def reconnect(self):
        self.close()
        self.connect()

    def run(self):

        if self.discard:
           self.inplace   = False
           self.overwrite = True
           self.log_back  = False

        if self.notify_only :
           self.log_back  = False

        self.logger.info("AMQP  broker(%s) user(%s) vhost(%s)" % (self.broker.hostname,self.broker.username,self.broker.path) )
        for k in self.exchange_key :
            self.logger.info("AMQP  input :    exchange(%s) topic(%s)" % (self.exchange,k) )
        if self.log_back :
            self.logger.info("AMQP  output:    exchange(%s) topic(%s)\n" % ('xlog','v02.log.#') )


        if not self.connected : self.connect()

        if not hasattr(self,'msg') :
           self.msg = sr_message(self.logger)

        self.msg.user         = self.amqp_user
        self.msg.report_publisher = self.amqp_log
        self.msg.logger       = self.logger


        while True :

             try  :
                  raw_msg = self.consumer.consume(self.queue)
                  if raw_msg == None : continue

                  self.logger.info("Received msg  %s" % vars(raw_msg))

                  # make use it as a sr_message

                  try :
                           self.msg.from_amqplib(raw_msg)
                  except :
                           self.msg.code    = 417
                           self.msg.message = "Expectation Failed : sumflg or partflg"
                           self.msg.log_error()
                           self.consumer.ack(raw_msg)
                           continue

                  # process message

                  processed = self.treat_message()

                  if processed :
                     self.consumer.ack(raw_msg)
             except (KeyboardInterrupt, SystemExit):
                 break                 
             except :
                 (type, value, tb) = sys.exc_info()
                 self.logger.error("Type: %s, Value: %s,  ..." % (type, value))
                 
    def myinit(self):
        self.bufsize       = 128 * 1024     # read/write buffer size

        self.protocol      = 'amqp'
        self.host          = 'dd.weather.gc.ca'
        self.port          = 5671
        self.amqp_user     = 'anonymous'
        self.amqp_passwd   = 'anonymous'
        self.vhost         = '/'

        if self.port == 5672 :
           self.broker     =  urllib.parse.urlparse('%s://%s:%s@%s%s'% \
                              (self.protocol,self.amqp_user,self.amqp_passwd,self.host,self.vhost))
        else:
           self.broker     =  urllib.parse.urlparse('%s://%s:%s@%s:%d%s'%\
                              (self.protocol,self.amqp_user,self.amqp_passwd,self.host,self.port,self.vhost))

        self.masks         = []             # All the masks (accept and reject)
        self.lock          = '.tmp'         # file send with extension .tmp for lock

        self.exchange      = 'xpublic'
        self.exchange_type = 'topic'
        self.exchange_key  = []

        self.topic_prefix  = 'v02.post'

        self.http_user     = None
        self.http_passwd   = None

        self.flatten       = '/'
        self.mirror        = False

        self.strip         = 0
        self.overwrite     = True
        self.inplace       = True
        self.log_back      = True
        self.message_ttl   = None
        
        self.readConfig()

        # if not set in config : automated queue name saved in queuefile

        if self.queue == None :
           self.queuefile = ''
           parts = self.config.split(os.sep)
           if len(parts) != 1 :  self.queuefile = os.sep.join(parts[:-1]) + os.sep

           fnp   = parts[-1].split('.')
           if fnp[0][0] != '.' : fnp[0] = '.' + fnp[0]
           self.queuefile = self.queuefile + '.'.join(fnp[:-1]) + '.queue'

           self.queuename()

    def queuename(self) :

        self.queue  = 'q_anonymous.dd_subscribe'
        if sys.version[:1] >= '3' :
           self.queue += '.' + str(random.randint(0,100000000)).zfill(8)
           self.queue += '.' + str(random.randint(0,100000000)).zfill(8)
        else :
           self.queue += '.' + string.zfill(random.randint(0,100000000),8)
           self.queue += '.' + string.zfill(random.randint(0,100000000),8)

        if os.path.isfile(self.queuefile) :
           f = open(self.queuefile)
           self.queue = f.read()
           f.close()
        else :
           f = open(self.queuefile,'w')
           f.write(self.queue)
           f.close()


    # url path will be replicated under odir (the directory given in config file)
    def mirrorpath(self, odir, url ):
        nodir = odir

        start = 3
        if self.strip > 0 :
           start = start + self.strip
           if start > len(parts)-1 : return nodir
        
        try :
              parts = url.split("/")
              for d in parts[start:-1] :
                  nodir = nodir + os.sep + d
                  if os.path.isdir(nodir) : continue
                  os.mkdir(nodir)
        except :
              self.logger.error("could not create or use directory %s" % nodir)
              return None

        return nodir

    # process individual url notification
    def treat_message(self):

        url = self.msg.urlstr

        # root directory where the product will be put
        odir = self.getMatchingMask(url)

        # no root directory for this url means url not selected
        if not odir : return True
        
        # notify_only mode : print out received message
        if self.notify_only :
           self.logger.info("%s %s" % (self.msg.notice,self.msg.hdrstr))
           return True
        # log what is selected
        else :
           self.logger.info("Received topic   %s" % self.msg.topic)
           self.logger.info("Received notice  %s" % self.msg.notice)
           self.logger.info("Received headers %s" % self.msg.headers)

        # remove flag not supported
        if self.msg.sumflg == 'R' :
           self.logger.info("Remove flag not supported")
           return True
        
        # root directory should exists
        if not os.path.isdir(odir) :
           self.logger.error("directory %s does not exist" % odir)
           return False

        # mirror mode True
        # means extend root directory with url directory
        nodir = odir
        if self.mirror :
           nodir = self.mirrorpath(odir,url)
           if nodir == None : return False

        # filename setting
        parts = url.split("/")
        fname = parts[-1]

        # flatten mode True
        # means use url to create filename by replacing "/" for self.flatten character
        if self.flatten != '/' :
           start = 3
           if self.strip > 0 :
              start = start + self.strip
              if start > len(parts)-1 :
                 fname = parts[-1]
              else :
                 fname = self.flatten.join(parts[start:])

        # setting filepath and temporary filepath
        opath = nodir + os.sep + fname

        # setting local_file and URL and how the file is renamed

        self.msg.set_new(self.inplace,opath)
        self.msg.headers['rename'] = opath

        # if local_file has same checksum nothing to do

        if not self.overwrite and self.msg.checksum_match() :
           self.msg.code    = 304
           self.msg.message = 'not modified'
           self.msg.log_info()

           # a part unmodified can make a difference
           if self.inplace and self.msg.in_partfile :
              file_reassemble(self.msg)

           file_truncate(self.msg)
           return True

        # download the file

        self.download(self.msg,url,self.http_user,self.http_passwd)
        return True

    def house_keeping(self):

        # Delayed insertion
        # try reassemble the file, conditions may have changed since writing

        if self.inplace and self.msg.in_partfile :
           self.msg.code    = 307
           self.msg.message = 'Temporary Redirect'
           self.msg.log_info()
           file_reassemble(self.msg)
           return True

        # announcing the download or insert

        if self.msg.partflg != '1' :
           if self.inplace : self.msg.change_partflg('i')
           else            : self.msg.change_partflg('p')

        #self.msg.set_topic_url('v02.post',self.msg.local_url)
        #self.msg.set_notice(self.msg.local_url,self.msg.time)
        self.msg.code    = 201
        self.msg.message = 'Downloaded'
        self.msg.log_info()
              
        # if we inserted a part in file ... try reassemble

        if self.inplace and self.msg.partflg != '1' :
           file_reassemble(self.msg)

        return True

    def download(self,msg,url,user=None,password=None) :

        if sys.version[:1] >= '3' :
           import urllib.request, urllib.error
           urllib_request = urllib.request
           urllib_error   = urllib.error
        else :
           import urllib2
           urllib_request = urllib2
           urllib_error   = urllib2

        # get the file, in case of error it will try three times.
        nb_try = 0
        while nb_try < 3:
            nb_try = nb_try + 1
            try :
                # create a password manager                
                if user != None :                          
                    # Add the username and password.
                    password_mgr = urllib_request.HTTPPasswordMgrWithDefaultRealm()
                    password_mgr.add_password(None, url, user, password)
                    handler = urllib_request.HTTPBasicAuthHandler(password_mgr)
                        
                    # create "opener" (OpenerDirector instance)
                    opener = urllib_request.build_opener(handler)
    
                    # use the opener to fetch a URL
                    opener.open(url)
    
                    # Install the opener.
                    # Now all calls to urllib2.urlopen use our opener.
                    urllib_request.install_opener(opener)

                # set a byte range to pull from remote file

                req   = urllib_request.Request(url)

                if msg.partflg == 'i' :
                   str_range = 'bytes=%d-%d'%(msg.offset,msg.offset+msg.length-1)
                   req.headers['Range'] = str_range
                   
                response = urllib_request.urlopen(req)

                self.write_to_file(response,msg)                    

                self.house_keeping()
                #self.logger.info('Download successful: %s', url)  
                
                #option to discard file
                if self.discard: 
                    try:
                        os.unlink(self.msg.local_file)
                        self.logger.info('Discard %s', self.msg.local_file)
                    except:
                        self.logger.error('Unable to discard %s', self.msg.local_file)
                else:
                    self.logger.info('Local file created: %s', self.msg.local_file)
                    
                break
            except (KeyboardInterrupt, SystemExit):
                 break                     
            except TimeoutException:                    
                self.logger.error('Download failed: %s', url)                    
                self.logger.error('Connection timeout')
            except urllib_error.HTTPError as e:
                self.logger.error('Download failed: %s', url)                    
                self.logger.error('Server couldn\'t fulfill the request. Error code: %s, %s', e.code, e.reason)
            except urllib_error.URLError as e:
                self.logger.error('Download failed: %s', url)                                    
                self.logger.error('Failed to reach server. Reason: %s', e.reason)            
            except:
                self.logger.error('Download failed: %s', url )
                self.logger.error('Uexpected error')              
                
            self.logger.info('Retry in 3 seconds...')
            time.sleep(3)

    def write_to_file(self,req,msg) :

        os.chdir(msg.new_dir)
        msg.local_file = msg.new_file

        # no locking if insert
        if msg.partflg != '1' and not msg.in_partfile :
           local_file = msg.local_file
        else :
           local_file = msg.local_file + self.lock
           if self.lock == '.' :
              token = msg.local_file.split(os.sep)
              local_file = os.sep.join(token[:-1]) + os.sep + '.' + token[-1]
              self.logger.debug("lock file = %s" % local_file)
           
        # download/write
        if not os.path.isfile(local_file) :
           fp = open(local_file,'w')
           fp.close

        fp = open(local_file,'r+b')
        if msg.local_offset != 0 : fp.seek(msg.local_offset,0)

        while True:
          chunk = req.read(msg.bufsize)
          if not chunk : break
          fp.write(chunk)

        fp.close()

        # unlock
        if local_file != msg.local_file :
           os.rename(local_file,msg.local_file)

    def readConfig(self):
        currentDir = '.'
        currentFileOption = 'NONE' 
        self.readConfigFile(self.config,currentDir,currentFileOption)

        if self.masks == [] :
            print("Error 5: accept is missing from config file")
            print("Try '%s --help' for more information." % self.pgm)
            sys.exit(3)

        if self.exchange_key == [] :
            print("Error 6: exchange_key is missing from config file")
            print("Try '%s --help' for more information." % self.pgm)
            sys.exit(3)


    def readConfigFile(self,filePath,currentDir,currentFileOption):
        
        def isTrue(s):
            if  s == 'True' or s == 'true' or s == 'yes' or s == 'on' or \
                s == 'Yes' or s == 'YES' or s == 'TRUE' or s == 'ON' or \
                s == '1' or  s == 'On' :
                return True
            else:
                return False

        try:
            config = open(filePath, 'r')
        except:
            (type, value, tb) = sys.exc_info()
            print("Type: %s, Value: %s" % (type, value))
            return 

        for line in config.readlines():
            words = line.split()
            if (len(words) >= 2 and not re.compile('^[ \t]*#').search(line)):
                try:
                    if   words[0] == 'accept':
                         cmask       = re.compile(words[1])
                         cFileOption = currentFileOption
                         if len(words) > 2: cFileOption = words[2]
                         self.masks.append((words[1], currentDir, cFileOption, cmask, True))
                    elif words[0] == 'reject':
                         cmask = re.compile(words[1])
                         self.masks.append((words[1], currentDir, currentFileOption, cmask, False))
                    elif words[0] == 'broker': 
                         self.broker = urllib.parse.urlparse(words[1])
                         ok, self.broker = self.validate_amqp_url(self.broker)
                         if not ok :
                            self.logger.error("broker is incorrect (%s)" % words[1])
                            continue
                         self.protocol = self.broker.scheme
                         self.host     = self.broker.hostname
                         if self.broker.port     != None : self.port        = int(self.broker.port)
                         if self.broker.username != None : self.amqp_user   = self.broker.username
                         if self.broker.password != None : self.amqp_passwd = self.broker.password
                         if self.broker.path     != None : self.vhost       = self.broker.path
                         self.logger.debug("%s:%s@%s:%d%s"%(self.amqp_user,self.amqp_passwd,self.host,self.port,self.vhost))
                         self.logger.debug("%s"%self.broker.geturl())

                    elif words[0] == 'directory': currentDir = words[1]
                    elif words[0] == 'protocol': self.protocol = words[1]
                    elif words[0] == 'host':
                         self.host = words[1]
                         if self.port == 5672 :
                            self.broker     =  urllib.parse.urlparse('%s://%s:%s@%s%s'% \
                              (self.protocol,self.amqp_user,self.amqp_passwd,self.host,self.vhost))
                         else :
                            self.broker     =  urllib.parse.urlparse('%s://%s:%s@%s:%d%s'%\
                              (self.protocol,self.amqp_user,self.amqp_passwd,self.host,self.port,self.vhost))
                         self.logger.warning("host %s" % words[1])
                         self.logger.warning("host option deprecated (but still working)")
                         self.logger.warning("use this instead :")
                         self.logger.warning("broker %s\n" % self.broker.geturl())
                         self.logger.debug("%s:%s@%s:%d%s"%(self.amqp_user,self.amqp_passwd,self.host,self.port,self.vhost))
                         self.logger.debug("%s"%self.broker.geturl())
                    elif words[0] == 'port':
                         self.port = int(words[1])
                         if self.port == 5672 :
                            self.broker     =  urllib.parse.urlparse('%s://%s:%s@%s%s'% \
                              (self.protocol,self.amqp_user,self.amqp_passwd,self.host,self.vhost))
                         else :
                            self.broker     =  urllib.parse.urlparse('%s://%s:%s@%s:%d%s'%\
                              (self.protocol,self.amqp_user,self.amqp_passwd,self.host,self.port,self.vhost))
                         self.logger.warning("port %s" % words[1])
                         self.logger.warning("port option deprecated (but still working)")
                         self.logger.warning("use this instead :")
                         self.logger.warning("broker %s" % self.broker.geturl())
                         self.logger.debug("%s:%s@%s:%d%s"%(self.amqp_user,self.amqp_passwd,self.host,self.port,self.vhost))
                         self.logger.debug("%s"%self.broker.geturl())
                    elif words[0] == 'amqp-user':
                         self.amqp_user = words[1]
                         if self.port == 5672 :
                            self.broker     =  urllib.parse.urlparse('%s://%s:%s@%s%s'% \
                              (self.protocol,self.amqp_user,self.amqp_passwd,self.host,self.vhost))
                         else :
                            self.broker     =  urllib.parse.urlparse('%s://%s:%s@%s:%d%s'%\
                              (self.protocol,self.amqp_user,self.amqp_passwd,self.host,self.port,self.vhost))
                         self.logger.debug("%s:%s@%s:%d%s"%(self.amqp_user,self.amqp_passwd,self.host,self.port,self.vhost))
                         self.logger.debug("%s"%self.broker.geturl())
                    elif words[0] == 'amqp-password':
                         self.amqp_passwd = words[1]
                         if self.port == 5672 :
                            self.broker     =  urllib.parse.urlparse('%s://%s:%s@%s%s'% \
                              (self.protocol,self.amqp_user,self.amqp_passwd,self.host,self.vhost))
                         else :
                            self.broker     =  urllib.parse.urlparse('%s://%s:%s@%s:%d%s'%\
                              (self.protocol,self.amqp_user,self.amqp_passwd,self.host,self.port,self.vhost))
                         self.logger.debug("%s:%s@%s:%d%s"%(self.amqp_user,self.amqp_passwd,self.host,self.port,self.vhost))
                         self.logger.debug("%s"%self.broker.geturl())
                    elif words[0] == 'vhost':
                         self.logger.warning("vhost option deprecated (but still working)")
                         self.logger.warning("use  option broker (default amqp://anonymous:anonymous@dd.weather.gc.ca:5672/' ")
                         self.vhost = words[1]
                         if self.port == 5672 :
                            self.broker     =  urllib.parse.urlparse('%s://%s:%s@%s%s'%\
                              (self.protocol,self.amqp_user,self.amqp_passwd,self.host,self.vhost))
                         else :
                            self.broker     =  urllib.parse.urlparse('%s://%s:%s@%s:%d%s'%\
                              (self.protocol,self.amqp_user,self.amqp_passwd,self.host,self.port,self.vhost))
                         self.logger.debug("%s:%s@%s:%d%s"%(self.amqp_user,self.amqp_passwd,self.host,self.port,self.vhost))
                         self.logger.debug("%s"%self.broker.geturl())
                    elif words[0] == 'lock': self.lock = words[1]

                    elif words[0] == 'exchange': self.exchange = words[1]
                    elif words[0] == 'exchange_type': 
                         if words[1] in ['fanout','direct','topic','headers'] :
                            self.exchange_type = words[1]
                         else :
                            self.logger.error("Problem with exchange_type %s" % words[1])

                    elif words[0] == 'exchange_key':
                         self.logger.warning("exchange_key %s" % words[1])
                         self.logger.warning("exchange_key option deprecated (but still working)")
                         self.logger.warning("use this instead :")
                         subtopic = words[1].replace('v00.dd.notify.','')
                         subtopic = subtopic.replace('v02.post.','')
                         self.logger.warning("subtopic %s\n" % subtopic)
                         self.exchange_key.append(self.topic_prefix+'.'+subtopic)

                    elif words[0] == 'topic':        self.exchange_key.append(words[1])
                    elif words[0] == 'topic_prefix': self.topic_prefix = words[1]
                    elif words[0] == 'subtopic':     self.exchange_key.append(self.topic_prefix+'.'+words[1])
                    elif words[0] == 'http-user': self.http_user = words[1]
                    elif words[0] == 'http-password': self.http_passwd = words[1]
                    elif words[0] == 'mirror': self.mirror = isTrue(words[1])
                    elif words[0] == 'flatten': self.flatten = words[1]

                    elif words[0] == 'durable': self.durable = isTrue(words[1])
                    elif words[0] == 'expire': self.expire = int(words[1]) * 60 * 1000
                    elif words[0] == 'strip': self.strip = int(words[1])
                    elif words[0] == 'overwrite': self.overwrite = isTrue(words[1])
                    #default is file is reassemble at subscribe level
                    #elif words[0] == 'inplace': self.inplace = isTrue(words[1])
                    elif words[0] == 'log_back': self.log_back = isTrue(words[1])
                    elif words[0] == 'queue': self.queue = words[1] 
                    elif words[0] == 'message-ttl': self.message_ttl = int(words[1]) * 60 * 1000
                    else:
                        self.logger.error("Unknown configuration directive %s in %s" % (words[0], self.config))
                        print("Unknown configuration directive %s in %s" % (words[0], self.config))
                except:
                    self.logger.error("Problem with this line (%s) in configuration file of client %s" % (words, self.name))
        config.close()
    
    def getMatchingMask(self, filename): 
        for mask in self.masks:
            if mask[3].match(filename) != None :
               if mask[4] : return mask[1]
               return None
        return None

    def validate_amqp_url(self,url):
        if not url.scheme in ['amqp','amqps'] :
           return False,url

        user = url.username
        pasw = url.password
        path = url.path

        rebuild = False
        if user == None  :
           user = self.amqp_user
           rebuild = True
        if pasw == None  :
           pasw = self.amqp_passwd
           rebuild = True
        if path == ''  :
           path = '/'
           rebuild = True

        if rebuild :
           urls = '%s://%s:%s@%s%s' % (url.scheme,user,pasw,url.netloc,path)
           url  = urllib.parse.urlparse(urls)

        return True,url


def help():     
    pgm = os.path.basename(sys.argv[0])
    #print chr(27)+'[1m'+'Script'+chr(27)+'[0m'
    print("\nUsage: ")
    print("\n%s [-n|--no-download] [-d|--download-and-discard] [-l|--log-dir] <config-file>" % pgm)
    print("\nConnect to an AMQP broker to subscribe to timely file update announcements.\n")
    print("Examples:\n")    
    print("%s subscribe.conf  # download files and display log in stdout" % pgm)
    print("%s -d subscribe.conf  # discard files after downloaded and display log in stout" % pgm)
    print("%s -l /tmp subscribe.conf  # download files,write log file in directory /tmp" % pgm)
    print("%s -n subscribe.conf  # get notice only, no file downloaded and display log in stout\n" % pgm)
    print("subscribe.conf file settings, MANDATORY ones must be set for a valid configuration:\n" +
          "\nAMQP broker connection:\n" +
          "\tbroker amqp{s}://<user>:<pw>@<brokerhost>[:port]/<vhost>\n" +
	  "\t\t(default: amqp://anonymous:anonymous@dd.weather.gc.ca/ ) \n" +
          "\t\tbroken out: protocol,amqp-user,amqp-password,host,port,vhost\n" +
          "\nAMQP Queue settings:\n" +
          "\tdurable       <boolean>      (default: False)\n" +
          "\texchange      <name>         (default: xpublic)\n" +
          "\texpire        <minutes>      (default: None)\n" +
          "\tmessage-ttl   <minutes>      (default: None)\n" +
          "\tqueue         <name>         (default: None)\n" +
          "\tsubtopic      <amqp pattern> (MANDATORY)\n" +
          "\t\t  <amqp pattern> = <directory>.<directory>.<directory>...\n" +
          "\t\t\t* single directory wildcard (matches one directory)\n" +
          "\t\t\t# wildcard (matches rest)\n" +
          "\ttopic_prefix  <amqp pattern> (invariant prefix, currently v02.post)\n" +
          "\nHTTP Settings:\n" +
          "\thttp-password <pw> (default: None)\n" +
          "\thttp-user   <user> (default: None)\n" +
          "\nLocal File Delivery settings:\n" +
          "\taccept    <regexp pattern> (MANDATORY)\n" +
          "\tdirectory <path>           (default: .)\n" +
          "\tflatten   <boolean>        (default: false)\n" +
          "\tlock      <.string>        (default: .tmp)\n" +
          "\tmirror    <boolean>        (default: false)\n" +
          "\treject    <regexp pattern> (optional)\n" +
          "\tstrip    <count> (number of directories to remove from beginning.)\n" +
	  "" )

def signal_handler(signal, frame):
    print('You pressed Ctrl+C!')
    #print('Resume in 5 seconds...')
    #time.sleep(5)
    sys.exit()
    #os.kill(os.getpid(),9)

def verify_version():    
    python_version = (2,6,0)
    if sys.version_info < python_version :
        sys.stderr.write("Python version higher than 2.6.0 required.\n")
        exit(1)
        
    amqplib_version = '1.0.0'   
    if amqp.connection.LIBRARY_PROPERTIES['library_version'] < amqplib_version:
        sys.stderr.write("Amqplib version %s or higher required.\n" % amqplib_version)        
        exit(1)
    
def main():

    ldir = None
    notice_only = False
    discard = False
    config = None
    
    #get options arguments
    try:
      opts, args = getopt.getopt(sys.argv[1:],'hl:dtn',['help','log-dir=','download-and-discard','no-download'])
    except getopt.GetoptError as err:    
      print("Error 1: %s" %err)
      print("Try '%s --help' for more information." % os.path.basename(sys.argv[0]))
      sys.exit(2)                    
    
    #validate options
    if opts == [] and args == []:
      help()  
      sys.exit(1)
    for o, a in opts:
      if o in ('-h','--help'):
        help()
        sys.exit(1)
      elif o in ('-n','--no-download'):
        notice_only = True        
      elif o in ('-l','--log-dir'):
        ldir = a       
        if not os.path.exists(ldir) :
          print("Error 2: specified logging directory does not exist.")
          print("Try '%s --help' for more information."% os.path.basename(sys.argv[0]))
          sys.exit(2)
      elif o in ('-d','--download-and-discard'):
        discard = True        
        
    #validate arguments
    if len(args) == 1:
      config = args[0]
      if not os.path.exists(config) :
         print("Error 3: configuration file does not exist.")
         sys.exit(2)
    elif len(args) == 0:
      help()  
      sys.exit(1)
    else:      
      print("Error 4: too many arguments given: %s." %' '.join(args))
      print("Try '%s --help' for more information." % os.path.basename(sys.argv[0]))
      sys.exit(2)            
             

    # logging to stdout
    LOG_FORMAT = ('%(asctime)s [%(levelname)s] %(message)s')

    if ldir == None :
       LOGGER = logging.getLogger(__name__)
       logging.basicConfig(level=logging.INFO, format=LOG_FORMAT)

    # user wants to logging in a directory/file
    else :       
       fn     = os.path.basename( config.replace(".conf","") )
       lfn    = fn + "_%s" % os.getpid() + ".log"
       lfile  = ldir + os.sep + lfn

       # Standard error is redirected in the log
       sys.stderr = open(lfile, 'a')

       # python logging
       LOGGER = None
       fmt    = logging.Formatter( LOG_FORMAT )
       hdlr   = logging.handlers.TimedRotatingFileHandler(lfile, when='midnight', interval=1, backupCount=5) 
       hdlr.setFormatter(fmt)
       LOGGER = logging.getLogger(lfn)
       LOGGER.setLevel(logging.INFO)
       LOGGER.addHandler(hdlr)

    # instanciate consumer

    consumer = ConsumerX(config,LOGGER)
    consumer.notify_only = notice_only
    consumer.discard = discard
    
    consumer.run()
    """
    while True:
         try:
                consumer.run()
         except :
                (type, value, tb) = sys.exc_info()
                LOGGER.error("Type: %s, Value: %s,  ..." % (type, value))
                time.sleep(10)
                pass
                
    """
    consumer.close()


if __name__ == '__main__':
    verify_version()
    signal.signal(signal.SIGINT, signal_handler)
    main()

